<!DOCTYPE html>
<html lang="en">
<head>
    <title>Gemini Live Voice to Text Realtime Stream</title>
    <!-- By Jim Salsman, April 2025. Released under the free MIT License. -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        /* General body styles */
        body {
            font-family: sans-serif;
            display: flex;
            flex-direction: column;
            height: 100vh;
            margin: 0;
            overflow: hidden; /* Prevent body from scrolling */
            background-color: #eee;    
        }

        /* Control bar styles */
        #controls {
            padding: 5px;
            /* Light background and border */
            background-color: lightcyan;
            border-bottom: 1px solid #ccc;
            text-align: center;
            position: relative;
        }
        #controls-content {
            display: flex;
            align-items: center;            
            justify-content: flex-end;
            width: 100%;            
            text-align: center; /* Center content horizontally */
            gap: 5px; /* Add gap between elements */
        }

        /* Start/Stop listening button */
        #toggleStream {
            padding: 10px 20px;
            font-size: 1em;
            cursor: pointer;
            background-color: #4c4;
            color: white;
            border: none;
            border-radius: 5px;
        }

        /* Send image button */
        #sendImage {   
            background-color: blue;
            display: none; /* Initially hidden */
            border: none;
            color: white;
            font-size: 1em;
            padding: 10px 20px;
            border-radius: 5px;
        }

        /* Send text button */
        #sendText {   
            background-color: darkorange;
            display: none; /* Initially hidden */
            border: none;
            color: white;
            font-size: 1em;
            padding: 10px 20px;
            border-radius: 5px;
        }

        /* Style for 'stop' state of the listening button */
        #toggleStream.stop {
            background-color: red;
        }

        /* Title style, floated to the left */
        #title {
            font-size: 1.2em;
            border-radius: 5px;
            margin-right: auto; /* Push the title to the far left */
        }

        /* Styles for debug input elements */
        #controls-content > label, #controls-content > input {
            margin-left: 5px;
        }

        /* Debug container style */
        #debugContainer {
            margin-right: auto; /* Push the container to the far left */
            text-align: center; /* Center the content */
        }

        /* Main output container styles */
        #output-container {
            flex-grow: 1; /* Take up remaining space */
            padding: 10px;
            padding-bottom: 30px;
            overflow-y: auto; /* Enable scrolling if content overflows */
            background-color: white;
            border: 1px solid lightgray;
            margin: 8px;
            border-radius: 5px;
            scroll-behavior: smooth; /* Smooth scroll behavior */
        }

        /* Output text style */
        #output {
            white-space: pre-wrap; /* Wrap long lines */
            word-wrap: break-word; /* Break long words onto multiple lines */
            font-family: sans-serif;
            font-size: 0.9em;
        }

        /* Styles to eliminate extra whitespace around output content */
        #output p {
            margin-bottom: -0.5em; /* Remove vertical whitespace */
            margin-top: -0.2em;
        }
        #output ul, #output ol, #output li {
            margin-top: -0.5em;
            margin-bottom: -0.5em; /* Remove vertical whitespace */
        }

        /* Styles to eliminate extra whitespace around <pre> output content */
        #output pre {
            margin: 0; /* Remove vertical whitespace */
            white-space: pre-wrap; /* Wrap long lines */
        }

        /* Styles for images in the output area */
        .output-image {
            max-height: 50vh;
            max-width: 90vw;
        }

        /* Output text styles */
        .error {
            color: red;
            font-weight: bold;
        }
        .info {
            color: blue;
            font-style: italic;
        }

        /* Styles for floating text input */
        #floatingTextInput {
            display: none; /* Initially hidden */
            position: fixed; /* Fixed position */
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%); /* Center */
            width: 80vw; /* 80% of viewport width */
            max-width: 600px; /* Max width */
            background-color: rgba(255, 255, 255, 0.95); /* Semi-transparent background */
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0px 0px 20px rgba(0, 0, 0, 0.3);
            z-index: 100; /* Ensure it's on top */
        }
        #textInput {            
            width: 100%;
            height: 10em; /* 10 lines */
            margin-bottom: 10px;
            resize: vertical; /* Allow vertical resize */
        }
    </style>    
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
</head>
<body>
    <div id="controls">
        <div id="controls-content">
            <h1 id="title">Gemini Live<br>Voice to Text Stream</h1>
            <div id="debugContainer">
                <a href="https://github.com/jsalsman/gemini-live"
                    target="_blank" style="text-decoration: none;">This
                    code is on GitHub.</a>
                <br><br>
                <input type="checkbox" id="debugCheckbox">
                <label for="debugCheckbox"><small>Debug<br>
                    &nbsp;&nbsp;to console</small></label>
            </div>
            <button id="toggleStream">Start<br>
                Listening</button>
            <button id="sendImage">Send<br>
                Image</button>
            <button id="sendText">Send<br>
                Text</button>
        </div>
    </div>

    <div id="output-container">
        <pre id="output"></pre>
    </div>

    <!-- Floating Text Input Area -->
    <div id="floatingTextInput">
        <textarea id="textInput"></textarea>
        <button>Send</button>
        <small>or Enter to send; use Shift+Enter for newlines</small>        
    </div>

    <script type="module">
        // Import necessary modules
        import { marked } from 'https://esm.sh/marked'; // Markdown
        import markedKatex from 'https://esm.sh/marked-katex-extension'; // LaTeX
        import { GoogleGenAI, Modality } from 'https://esm.run/@google/genai';

        // Docs: https://ai.google.dev/gemini-api/docs/live
        // https://googleapis.github.io/js-genai/main/index.html
        // https://github.com/googleapis/js-genai

        // --- Configuration ---
        let API_KEY = null;
        // you can hardcode an API key here if you want to serve just this file from localhost
        
        const MODEL_NAME = "gemini-2.0-flash-live-001"; // Realtime/Live model
        const TARGET_SAMPLE_RATE = 16000; // Gemini requires 16kHz audio
        const SYSTEM_PROMPT = 'Please be a helpful assistant and kind conversationalist. ' +
            'Respond to the user(s) directly by answering with relevant information ' +
            'in full detail. When you do not have relevant information, use your ' +
            'Google Search tool to search for it. You can use Markdown and LaTeX. ' +
            "Do not merely describe the user(s)' utterances. Do not include timestamps.";
        const INSTRUCTIONS = "**Instructions:**\n\nYou can interact with Gemini Live " +
            "by speaking, uploading image files or from your camera, or typing or " +
            "pasting text. You can search the web, ask for Python code to be executed, " +
            "and see images like graphs from Matplotlib, as well as LaTeX mathematics " +
            'typesetting.\n\nTry asking, "What can you do?"\n\n';

        // --- DOM Elements ---
        const controls = document.getElementById('controls');
        const toggleButton = document.getElementById('toggleStream');
        const outputArea = document.getElementById('output');
        const sendImageButton = document.getElementById('sendImage');
        const sendTextButton = document.getElementById('sendText');
        const debugContainer = document.getElementById('debugContainer');
        const floatingTextInput = document.getElementById('floatingTextInput');
        const textInput = document.getElementById('textInput');

        // --- State Variables ---
        let genAI;
        let liveSession = null;
        let mediaStream = null;
        let audioContext = null;
        let audioSource = null;
        let audioWorkletNode = null;
        let isStreaming = false;        
        let currentTurnText = "";
        let totalBytesSent = 0;
        let contextTurns = [];

        function getCookie(name) {
            const value = `; ${document.cookie}`;
            const parts = value.split(`; ${name}=`);
            if (parts.length === 2) return parts.pop().split(';').shift();
        }

        function checkApiKey() {
             if (API_KEY) {
                return true;
             }
             API_KEY = getCookie('gemini_api_key');
             if (!API_KEY) {
                liveOutput("API Key not set. Please reload this page to enter it.", 'error');
                return false;
            }
            return true;
        }
        
        // --- Debug Checkbox Logic ---
        const debugCheckbox = document.getElementById('debugCheckbox');
        let debugMode = false; // Flag to control debug logging
        // Add an event listener to the checkbox to toggle debug mode
        debugCheckbox.addEventListener('change', () => {
            debugMode = debugCheckbox.checked;
        });

        marked.use(markedKatex({ throwOnError: false })); // LaTeX

        function liveOutput(text, type = 'text') {
            const line = document.createElement('div');
            line.innerHTML = marked.parse(text).replace(
                '<a href=', '<a TARGET="_blank" href='); // all links open in new tab
            line.classList.add('output');
            if (type === 'error') {
                line.classList.add('error');
                console.error(text);
            } else if (type === 'info') {
                line.classList.add('info');
                if (debugMode) {
                    console.log(text);
                }
            } else {
                 line.classList.add('text');
            }
            if (type !== 'error' && type !== 'info') { // Only log "Gemini:" if not error or info
                if (debugMode) {
                    console.log("Gemini:", text.replace('\n&nbsp;', ''));
                }
            }
            outputArea.appendChild(line);
            outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;
        }

        function arrayBufferToBase64(buffer) {
            let binary = "";
            const bytes = new Uint8Array(buffer);
            const len = bytes.byteLength;
            for (let i = 0; i < len; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return window.btoa(binary);
        }

        // --- Audio Worklet ---
        const AudioRecordingWorklet = `
            class AudioProcessingWorklet extends AudioWorkletProcessor {
                buffer = new Int16Array(2048);
                bufferWriteIndex = 0;

                constructor() {
                    super();
                }

                process(inputs, outputs, parameters) {
                    if (inputs.length > 0 && inputs[0].length > 0) {
                        const channelData = inputs[0][0];
                        this.processChunk(channelData);
                    }
                    return true; // Keep processor alive
                }

                sendAndClearBuffer() {
                    if (this.bufferWriteIndex > 0) {
                        const dataToSend = this.buffer.slice(0, this.bufferWriteIndex);
                        this.port.postMessage({
                            eventType: "audioData",
                            audioData: dataToSend.buffer // Send ArrayBuffer
                        }, [dataToSend.buffer]); // Transfer buffer ownership for efficiency
                        this.bufferWriteIndex = 0;
                    }
                }

                processChunk(float32Array) {
                    for (let i = 0; i < float32Array.length; i++) {
                        const clampedValue = Math.max(-1.0, Math.min(1.0, float32Array[i]));
                        const int16Value = Math.floor(clampedValue * 32767);
                        this.buffer[this.bufferWriteIndex++] = int16Value;
                        if (this.bufferWriteIndex >= this.buffer.length) {
                            this.sendAndClearBuffer();
                        }
                    }
                }
            }
            registerProcessor('audio-processing-worklet', AudioProcessingWorklet);
        `;

        // --- Volume Meter Worklet --
        const VolumeMeterWorklet = `
        class VolumeMeter extends AudioWorkletProcessor {
            constructor() {
                super();
                this.volume = 0;
            }
            
            process(inputs) {
                const input = inputs[0];
                if (input.length > 0 && input[0].length > 0) {
                    // Calculate RMS (Root Mean Square)
                    let sumOfSquares = 0.0;
                    for (const sample of input[0]) {
                        sumOfSquares += sample * sample;
                    }
                    const rms = Math.sqrt(sumOfSquares / input[0].length);
                    
                    // Convert RMS to a linear scale (0.0 to 1.0)
                    this.volume = Math.min(1.0, rms * 10); // Adjust multiplier as needed for sensitivity
                    // Post a message to main thread with the volume level
                    this.port.postMessage({ volume: this.volume });
                } else {
                    this.volume = 0;
                }
                
                return true;
            }
        }
        registerProcessor('volume-meter', VolumeMeter);
        `;
        let volumeWorkletNode;

        // Function to update the VU meter's background color
        function updateVuMeter(volume) {
            const darkness = Math.round(volume * 150); // Adjust multiplier for sensitivity
            const newRed = Math.max(0, 224 - darkness); // Base red: 224
            const newGreen = Math.max(0, 255 - darkness); // Base green: 255
            const newBlue = Math.max(0, 255 - darkness); // Base blue: 255
            controls.style.backgroundColor = `rgb(${newRed}, ${newGreen}, ${newBlue})`;
        }

        // --- Core Streaming Logic ---

        async function startStreaming() {
            if (isStreaming) return;
            if (!checkApiKey()) {
                return;
                
            }

            isStreaming = true; // Set streaming flag early
            toggleButton.innerHTML = "Stop<br>Listening";
            toggleButton.classList.add('stop');
         
            try {
                // Step 1: Initialize GoogleGenAI Client
                genAI = new GoogleGenAI({ apiKey: API_KEY });
                try {
                    const modelInfo = await genAI.models.get({ model: MODEL_NAME });
                    if (debugMode) {
                        console.log("Gemini model info:", modelInfo);
                    }
                } catch (testError) {
                    // Clear the cookie
                    document.cookie = 'gemini_api_key=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;';
                    // Redirect to root
                    window.location.href = '/';
                    return;
                }

                // Step 2: Get Microphone Access
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: TARGET_SAMPLE_RATE,
                        echoCancellation: false,
                        noiseSuppression: true
                    }
                });
                
                // Step 3: Create Audio Context and Source
                audioContext = new AudioContext({ sampleRate: TARGET_SAMPLE_RATE });
                // Resume context if suspended (often needed after page load)
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }
                audioSource = audioContext.createMediaStreamSource(mediaStream);
                 // Create a gain node to split the audio signal
                 const splitter = audioContext.createChannelSplitter(2);
                const merger = audioContext.createChannelMerger(2);
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 1; // No gain change
               
                // Step 4: Set up Audio Worklet
                const workletBlob = new Blob([AudioRecordingWorklet], { type: 'application/javascript' });
                const workletURL = URL.createObjectURL(workletBlob);
                try {
                    await audioContext.audioWorklet.addModule(workletURL);
                } catch (e) {
                    liveOutput(`Error adding AudioWorklet module: ${e.message}. Make sure you are serving this page over HTTPS or localhost.`, 'error');
                    throw e; // Re-throw to be caught by outer catch
                }
                audioWorkletNode = new AudioWorkletNode(audioContext, 'audio-processing-worklet');

                // Add the volume meter worklet
                const volumeWorkletBlob = new Blob([VolumeMeterWorklet], { type: 'application/javascript' });
                const volumeWorkletURL = URL.createObjectURL(volumeWorkletBlob);
                try {
                    await audioContext.audioWorklet.addModule(volumeWorkletURL);
                } catch (e) {
                    liveOutput(`Error adding VolumeMeter AudioWorklet module: ${e.message}. Make sure you are serving this page over HTTPS or localhost.`, 'error');
                    throw e; // Re-throw to be caught by outer catch
                }
                volumeWorkletNode = new AudioWorkletNode(audioContext, 'volume-meter');

                // Connect audio nodes: Mic Source -> Gain Node -> Volume Worklet
                audioSource.connect(gainNode);
                gainNode.connect(volumeWorkletNode);
                
                //connect audio nodes: Gain Node -> split -> merge -> Gemini Worklet
                gainNode.connect(splitter);
                splitter.connect(merger, 0, 0); // Connect channel 0 to input 0
                splitter.connect(merger, 0, 1); // Connect channel 0 to input 1
                merger.connect(audioWorkletNode);

                volumeWorkletNode.connect(audioWorkletNode);
               
                // Step 5: Connect to Gemini Live API
                // Assign to liveSession *after* connection is successful

                liveSession = await genAI.live.connect({
                    model: MODEL_NAME,
                    audioConfig: { targetSampleRate: TARGET_SAMPLE_RATE},
                    systemInstruction: SYSTEM_PROMPT,
                    inputAudioTranscription: true, // Enable audio transcription, not working; being worked on now
                    config: {
                        responseModalities: [Modality.TEXT],
                        tools: [
                            {codeExecution: {}},
                            {googleSearch: {}},
                            ],
                        generationConfig: {
                            temperature: 0,
                            seed: 42,
                        },
                        temperature: 0, // these are also accepted here, so why not? lol
                        seed: 42,
                    },
                    callbacks: {
                        onopen: () => {
                            // The connection to Gemini is open; audio processing and sending can begin
                            debugContainer.style.display = 'none'; // Hide controls
                            sendImageButton.style.display = 'inline-block'; // Show the button
                            sendTextButton.style.display = 'inline-block';
                            //liveOutput("Connected to Gemini. Listening...", 'info');
                        },
                        onmessage: (message) => {
                            if (debugMode) {
                                console.log("Debug message:", message);
                            }

                            if (message.serverContent?.modelTurn?.parts?.length > 0) {  
                                const part = message.serverContent.modelTurn.parts[0];

                                if (part.inlineData && part.inlineData.mimeType.startsWith('image/')) {
                                    try {
                                        // Handle image data
                                        const img = document.createElement('img');       
                                        img.classList.add('output-image'); // Add CSS class for styling
                                        img.src = `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`;
                                        liveOutput("Image from Gemini:", "info")
                                        outputArea.appendChild(img);
                                        outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;
                                        // append model turn to context
                                        if (!part.inlineData.mimeType.startsWith('image/gif')) { // .gif files cause bugs
                                            contextTurns.push({role: 'model', parts: [{inlineData: 
                                                {'mimeType': part.inlineData.mimeType, 'data': part.inlineData.data}}]});
                                        }
                                    } catch (renderError) {
                                        liveOutput(`Error rendering image: ${renderError.message}`, 'error');
                                    }
                                } else if (part.executableCode) {
                                    liveOutput("**Python code:**\n```\n" + part.executableCode.code + " ```");
                                    // append model turn to context
                                    contextTurns.push({role: 'model', parts: [{text: part.executableCode.code}]});
                                } else if (part.codeExecutionResult) {
                                    const result = part.codeExecutionResult;
                                    liveOutput("**Output:** (" + result.outcome + ")\n``` \n" + result.output + " ```");
                                    // append model turn to context
                                    contextTurns.push({role: 'model', parts: [{text: result.outcome + "\n\n" + result.output}]});
                                } else {
                                    const textPart = message.serverContent.modelTurn.parts.find(part => part.text);
                                    if (textPart && textPart.text.trim()) {
                                        currentTurnText += textPart.text;
                                    }
                                }

                            }
                            if (message.serverContent?.turnComplete || message.generationComplete) {
                                // Finalize text when a turn was completed
                                if (currentTurnText.trim().length > 0) {
                                    liveOutput(currentTurnText + "\n&nbsp;");
                                    // append model turn to context
                                    contextTurns.push({role: 'model', parts: [{text: currentTurnText}]});
                                }
                                currentTurnText = ""; // Reset for next turn
                            }
                            if (message.error) {
                                liveOutput(`Gemini Error: ${message.error.message || JSON.stringify(message.error)}`, 'error');
                            }
                        },
                        onerror: (errorEvent) => {
                            // Use errorEvent directly if it's an Error object, otherwise stringify
                            const errorMessage = errorEvent instanceof Error ? errorEvent.message : JSON.stringify(errorEvent);
                            liveOutput(`Gemini connection error: ${errorMessage}`, 'error');
                            stopStreaming(); // Stop on connection error
                        },
                        onclose: (closeEvent) => {
                            // Only call stopStreaming if the closure was unexpected while we were actively streaming
                            if (isStreaming) {
                                liveOutput("Connection closed unexpectedly.", 'error');
                                stopStreaming();
                            }
                         },
                    },
                });
                // send prior context if any
                if (contextTurns.length) {
                    liveOutput(`Sending ${contextTurns.length} turns of prior context above.`, 'info');
                    contextTurns.push({role: 'user', parts: [{text: "Please summarize our discussion so far."}]})
                    await liveSession.sendClientContent({
                        turns: contextTurns,
                        turnComplete: true
                    });
                }
                liveOutput("Connected to Gemini. Listening...", 'info');

                // Step 6: Handle Audio Data from Worklet
                audioWorkletNode.port.onmessage = (event) => {
                    // Check if it's audio data, if the session exists, and if we are still streaming
                    if (event.data.eventType === 'audioData' && liveSession && isStreaming) {
                        const audioDataBuffer = event.data.audioData;
                        const base64AudioData = arrayBufferToBase64(audioDataBuffer);
                        
                        try {
                            // Send audio chunk to Gemini
                            liveSession.sendRealtimeInput({
                                media: {
                                    data: base64AudioData,
                                    mimeType: `audio/pcm;rate=${TARGET_SAMPLE_RATE}`
                                }
                            });
                            totalBytesSent += audioDataBuffer.byteLength;
                            if (debugMode && (totalBytesSent - 4096) % 163840 === 0) { // every ~5 seconds
                                console.log("Audio sent", totalBytesSent, "bytes,",
                                    Math.round(totalBytesSent / 32000), "seconds");
                            }
                        } catch (sendError) {
                            liveOutput(`Error sending audio data: ${sendError.message}`, 'error');
                            // Optionally stop streaming if sending fails repeatedly
                            // stopStreaming();
                        }
                    }
                };

                // Handle volume events
                volumeWorkletNode.port.onmessage = (event) => {
                    if (event.data.volume !== undefined) {
                       updateVuMeter(event.data.volume);
                    }
                };

            } catch (error) {
                liveOutput(`Error starting stream: ${error.message || error}`, 'error');
                await stopStreaming(); // Ensure cleanup happens even on startup error
            }
        }

        async function stopStreaming() {
            // Prevent multiple stop calls overlapping
            if (!isStreaming && !liveSession && !mediaStream && !audioContext) {
                console.log("Stop called but already stopped/cleaned up.");
                return;
            } 
            const wasStreaming = isStreaming; // Keep track if we were actively streaming
            isStreaming = false; // Set flag immediately to stop sending data

            // Step 7: Close Gemini Session
            if (liveSession) {
                try {
                    liveSession.close();
                } catch (e) {
                    liveOutput(`Error closing Gemini session: ${e.message}`, 'error');
                } finally {
                    liveSession = null;
                }
            }

            // Step 8: Stop Audio Processing
            if (audioWorkletNode) {
                audioWorkletNode.disconnect();
                audioWorkletNode = null;
            }
            // disconnect the volume meter node
            if (volumeWorkletNode) {
                volumeWorkletNode.disconnect();
                
                volumeWorkletNode = null;
                
            }
            if (audioSource) {
                audioSource.disconnect();
                audioSource = null;
            }            
            // Step 9: Stop Media Stream Tracks
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            // Step 10: Close Audio Context
            if (audioContext && audioContext.state !== 'closed') {
                try {
                    await audioContext.close();
                } catch (e) {
                    liveOutput(`Error closing AudioContext: ${e.message}`, 'error');
                } finally { audioContext = null;
                }
            }

            // Step 11: Reset UI
            toggleButton.innerHTML = "Start<br>Listening";
            toggleButton.classList.remove('stop');
            sendTextButton.style.display = 'none'; // Hide the text button
            sendImageButton.style.display = 'none'; // Hide the image button
            debugContainer.style.display = 'inline-block'; // Restore controls
            genAI = null; // Clear the client instance
            if (wasStreaming) {
                liveOutput("Stopped listening.\n", 'info');
            } 
        }

        // --- Event Listener ---
        toggleButton.addEventListener('click', () => {
            if (isStreaming) {
                stopStreaming();
            } else {
                startStreaming();
            }
        });

        // "Send Image" button event listener
        document.getElementById('sendImage').addEventListener('click', () => {
            // Create a file input element
            const input = document.createElement('input');
            input.type = 'file';
            input.accept = 'image/*'; // Accept only image files
            
            // Add event listener for file selection
            input.addEventListener('change', (event) => {
                const file = event.target.files[0]; 
                if (file) {                    
                    liveOutput(`Processing image: ${file.name}`, 'info');
                    const reader = new FileReader();
                    reader.onload = async () => {
                        try {
                            let base64Data = reader.result.split(',')[1];
                            let mimeType = file.type;

                            // Downscale if necessary
                            if (base64Data.length > 250 * 1024 * 0.75) { // Approximate base64 size
                                const downscaled = await downscaleImage(base64Data, mimeType, 250 * 1024);
                                base64Data = downscaled.base64;
                                mimeType = downscaled.mimeType;
                                liveOutput(`Image downscaled to ${(base64Data.length * 0.75 / 1024).toFixed(2)}KB`, 'info');
                            }

                            // Display image in output area
                            const img = document.createElement('img');
                            img.classList.add('output-image');
                            img.src = `data:${mimeType};base64,${base64Data}`;
                            outputArea.appendChild(img);
                            outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;

                            // Send image to stream
                            sendImageToStream(base64Data, mimeType);

                        } catch (error) {
                            liveOutput(`Error processing image: ${error.message}`, 'error');
                        }
                    };
                    reader.onerror = (error) => { liveOutput(`Failed to read file: ${error}`, 'error'); };
                    reader.readAsDataURL(file);
                }
            });
            
            // Trigger the file input dialog
            input.click();
        });

        // Helper function for downscaling
        function downscaleImage(base64, mimeType, maxSize) {
            return new Promise((resolve, reject) => {
                const img = new Image();
                img.onload = () => {
                    let width = img.width;
                    let height = img.height;
                    let newWidth = width;
                    let newHeight = height;

                    // Calculate scaling factor to fit within maxSize
                    const canvas = document.createElement('canvas');
                    const ctx = canvas.getContext('2d');

                    // Binary search for the right quality
                    let low = 0.1;
                    let high = 1.0;
                    let quality = 0.8;  // Initial guess
                    let result = null;

                    for (let i = 0; i < 10; i++) {  // 10 iterations of binary search
                        canvas.width = newWidth;
                        canvas.height = newHeight;
                        ctx.drawImage(img, 0, 0, newWidth, newHeight);
                        const newBase64 = canvas.toDataURL(mimeType, quality).split(',')[1];
                        const newSize = newBase64.length * 0.75;

                        if (newSize <= maxSize) {
                            result = { base64: newBase64, mimeType: mimeType };
                            low = quality;  // Try for better quality
                        } else {
                            high = quality;
                            // Scale down image dimensions to reduce size.
                            const scaleFactor = Math.sqrt(maxSize / newSize);
                            newWidth = Math.floor(newWidth * scaleFactor);
                            newHeight = Math.floor(newHeight * scaleFactor);
                        }
                        quality = (low + high) / 2;
                    }

                    if (result) {
                        resolve(result);
                    } else {
                        // Should not happen after scaling image dimensions
                        reject(new Error("Could not downscale image to the required size."));
                    }
                };
                img.onerror = () => reject(new Error("Failed to load image for downscaling."));
                img.src = `data:${mimeType};base64,${base64}`;
            });
        }

        // Function to send the image to the stream
        async function sendImageToStream(base64Image, mimeType = 'image/jpeg') {
            if (!liveSession || !isStreaming) {
                liveOutput('Error: Live session not active. Please start listening first.', 'error');
                return;
            }
            try {
                // Debug information
                if (debugMode) {
                    console.log(`Sending image (${(base64Image.length * 0.75 / 1024).toFixed(2)}KB) with MIME type: ${mimeType}`);
                }
                
                // Send the image as a completed turn
                await liveSession.sendClientContent({
                    turns: [{
                        role: 'user',
                        parts: [{
                            inlineData: {
                                mimeType: mimeType,
                                data: base64Image
                            }
                        }]
                    }],                    
                    turnComplete: true
                });
                liveOutput('Image sent successfully! Waiting for Gemini to process...', 'info');
                // append user turn to context
                contextTurns.push({role: 'user', parts: [{inlineData: {'mimeType': mimeType, 'data': base64Image}}]});
            } catch (error) {
                // Handle the error appropriately
                liveOutput(`Error sending image: ${error.message || JSON.stringify(error)}`, 'error');                
            }
        }

        // Function to submit text input from the floating input area
         function submitTextInput() {
            const text = textInput.value;
            floatingTextInput.style.display = 'none';
            textInput.value = "";
            if (text.trim() !== "") { // Check for empty input
                sendTextToStream(text); // Send the text to the stream
            }
        }

        // If using Enter key in the textarea:
        textInput.addEventListener('keydown', (event) => {
            if (event.key === 'Enter' && !event.shiftKey) { // Enter without Shift
                event.preventDefault(); // Prevent default Enter behavior (newline)
                submitTextInput();
            }
        });

        async function sendTextToStream(text) {
            if (!liveSession || !isStreaming) {
                liveOutput('Error: Live session not active. Please start listening first.', 'error');
                return;
            }
            try {
                await liveSession.sendClientContent({
                    turns: [{
                        role: 'user',
                        parts: [{
                            text: text
                        }]
                    }],
                    turnComplete: true
                });
                liveOutput('Sent: ' + text, 'info');
                // append user turn to context
                contextTurns.push({role: 'user', parts: [{text: text}]});
            } catch (error) {
                liveOutput(`Error sending text: ${error.message || JSON.stringify(error)}`, 'error');
            }
        }

        // Event listener for the Send button in the floating text input
        document.querySelector('#floatingTextInput button').addEventListener('click', () => {
            submitTextInput();
        });
    
        // Add a click event listener to the sendTextButton
        sendTextButton.addEventListener('click', () => {
            floatingTextInput.style.display = 'block'; // Display the input area
            textInput.focus(); // Focus on the text input area
        });

        // Event listener to hide floatingTextInput on outside click
        document.addEventListener('click', (event) => {
            if (
                floatingTextInput.style.display === 'block' &&
                !floatingTextInput.contains(event.target) &&
                event.target !== sendTextButton // Check if the click is NOT on the sendTextButton
            ) {
                floatingTextInput.style.display = 'none';
            }
        });

        if (checkApiKey()) {
            const instructions = document.createElement('div');
            instructions.innerHTML = marked.parse(INSTRUCTIONS)
            outputArea.appendChild(instructions);
            outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;
            liveOutput("Click 'Start Listening' to begin.", 'info');
        }

        // Add safety net for page unload
        window.addEventListener('beforeunload', () => {
            if (isStreaming) {
                stopStreaming(); // Attempt cleanup if user navigates away
            }
        });
    </script>
</body>
</html>
